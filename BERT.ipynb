{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:51:51.793183900Z",
     "start_time": "2024-02-19T15:51:51.771367400Z"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#Separte columns in datasets a write changes to a new .csv\n",
    "original_file_path = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\training.csv'\n",
    "df = pd.read_csv(original_file_path, delimiter='\\t', encoding='utf-8')\n",
    "training1 = 'C:\\\\Desktop\\\\training\\\\TRAINING\\\\training1.csv'\n",
    "df.to_csv(training1, index=False, sep=',')\n",
    "\n",
    "original_file_path = 'C:\\\\Desktop\\\\test\\\\Test.csv'\n",
    "df = pd.read_csv(original_file_path, delimiter='\\t', encoding='utf-8')\n",
    "test1 = 'C:\\\\Desktop\\\\test\\\\test1.csv'\n",
    "df.to_csv(test1, index=False, sep=',')\n",
    "\n",
    "# combine test1.csv with test_labels.txt\n",
    "testing_df = pd.read_csv('C:\\\\Desktop\\\\test\\\\test1.csv')\n",
    "\n",
    "# Open the text file and read the additional values\n",
    "additional_values = []\n",
    "with open('C:\\\\Desktop\\\\test_labels.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Now using '\\t' to split since the format is \"filename    misogynous    shaming    stereotype    objectification    violence\"\n",
    "        values = line.strip().split('\\t')\n",
    "        # Omit the filename from the values to be added to the DataFrame\n",
    "        additional_values.append(values[1:])  # Skip the filename\n",
    "\n",
    "# Convert additional_values to a DataFrame\n",
    "additional_df = pd.DataFrame(additional_values, columns=['misogynous', 'shaming', 'stereotype', 'objectification', 'violence'])\n",
    "\n",
    "# Ensure the correct data types, assuming all additional columns should be integers\n",
    "additional_df = additional_df.astype(int)\n",
    "\n",
    "# Concatenate the original testing_df with the additional_df\n",
    "final_df = pd.concat([testing_df, additional_df], axis=1)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "final_df.to_csv('C:\\\\Desktop\\\\updated_testing.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:53:47.530044700Z",
     "start_time": "2024-02-19T15:53:47.416319600Z"
    }
   },
   "id": "2751701365bc33b1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Reads CSV file to pandas DataFrame, extracts texts + labels into lists + returns them. load training + testing data\n",
    "def load_dataset(file_path):\n",
    "    df1 = pd.read_csv(file_path)\n",
    "    texts = df1['Text Transcription'].tolist()\n",
    "    labels = df1['misogynous'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "#Initializes the BERT tokenizer, which converts text into tokens that can be fed to the BERT model.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_texts, train_labels = load_dataset('C:\\\\Desktop\\\\training\\\\TRAINING\\\\training1.csv')\n",
    "test_texts, test_labels = load_dataset('C:\\\\Desktop\\\\updated_testing.csv') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:53:47.976642700Z",
     "start_time": "2024-02-19T15:53:47.678870700Z"
    }
   },
   "id": "b549c8ff28cd5aee"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    " #  tokenizes the list texts, adding necessary padding and truncation, to create a uniform input size. It returns PyTorch tensors.\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "#Tokenizes the training and testing text data.\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:53:56.731380Z",
     "start_time": "2024-02-19T15:53:48.724323700Z"
    }
   },
   "id": "2ea6aa764a68a9ac"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\rober\\PycharmProjects\\Misogyny-Identification\\venv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:54:03.819964600Z",
     "start_time": "2024-02-19T15:53:59.168398200Z"
    }
   },
   "id": "7b2e7cc30c199a1f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:54:20.654287300Z",
     "start_time": "2024-02-19T15:54:20.617128600Z"
    }
   },
   "id": "5f6f70d14b6d0d75"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust epochs based on your dataset and needs\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:54:35.670727500Z",
     "start_time": "2024-02-19T15:54:35.654486200Z"
    }
   },
   "id": "112c46add2e65f13"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "# Print classification report and accuracy\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('your_model_directory')\n",
    "tokenizer.save_pretrained('your_model_directory')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T15:53:35.295164100Z",
     "start_time": "2024-02-19T15:53:35.259959800Z"
    }
   },
   "id": "a3c506e72fe4f829"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe6b7db6772717a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
